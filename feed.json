{"title":"Yikun","description":null,"language":null,"link":"http://yikun.github.io","pubDate":"Tue, 27 Apr 2021 03:37:01 GMT","lastBuildDate":"Mon, 02 Aug 2021 03:30:02 GMT","generator":"hexo-generator-json-feed","webMaster":"Yikun","items":[{"title":"如何在上游贡献代码（Github篇）？","link":"http://yikun.github.io/2021/04/27/如何在上游贡献代码（Github篇）？/","description":"写给那些刚接触github和开源贡献的你们。","pubDate":"Tue, 27 Apr 2021 03:37:01 GMT","guid":"http://yikun.github.io/2021/04/27/如何在上游贡献代码（Github篇）？/","category":"OpenSource"},{"title":"大文件在Github和Gitee上传的建议","link":"http://yikun.github.io/2020/11/24/大文件在Github和Gitee上传的建议/","description":"本文详细解释了在github及gitee上对大文件处理的限制以及解决方案。 Github和Gitee文件大小限制 托管类型 单文件限制 单仓库限制 LFS单文件限制 LFS单账户限制 Github 100MB 建议小于1GB，强烈建议小于5GB 2GB 1GB Gitee 50MB 500MB 仅对企业付费用户开放 仅对企业付费用户开放 简单说，如果gitee通过非LFS方式，上传了100MB以上的文件，那么github无法镜像。gitee通过LFS方式，上传了100MB以上的文件，最大不能超过2GB，且总和不能超过1GB。否则会出现this exceeds GitHub&#39;s file size limit of 100.00 MB错误： 123456789101112131415root@yikun-x86:~/yikun/bigfile# git push origin mainEnumerating objects: 8, done.Counting objects: 100% (8/8), done.Delta compression using up to 4 threadsCompressing objects: 100% (5/5), done.Writing objects: 100% (6/6), 120.25 MiB | 191.00 KiB/s, done.Total 6 (delta 1), reused 0 (delta 0)remote: Resolving deltas: 100% (1/1), done.remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.remote: error: Trace: 58b4554c41925fcfb5dba5ec99aebb5ef9fab8d092461cb8ed321578a4fa178eremote: error: See http://git.io/iEPt8g for more information.remote: error: File boost_1_72_0.tar.gz is 120.72 MB; this exceeds GitHub&apos;s file size limit of 100.00 MBTo github.com:Yikun/bigfile.git ! [remote rejected] main -&gt; main (pre-receive hook declined)error: failed to push some refs to &apos;git@github.com:Yikun/bigfile.git&apos; 1. 如何处理大文件（超过100MB的文件）任何方案，都需要处理最近一次commit（通过删除或者LFS改造）和历史所有commits的提交（通过bfg） 第一步：处理当前大文件（最近一次commit） 找到大文件。通过git big-files和git blob-find找到所有出现问题的分支。 处理大文件：[推荐] 方案一（删除大文件，并保留历史提交）：通过自动化下载和脚本的方式，此步完成后，所有的大文件在lastest commit都被清除。方案二（利用LFS改造大文件，并保留历史提交）：lfs方式改造大文件。在每个分支通过git lfs方式进行改造，此步完成后，所有的大文件在lastest commit都改造为lfs方式。 第二步：处理历史大文件（历史所有commits） 清除历史大文件。利用bfg工具，清除所有历史大文件提交记录。（注意：此步会重新提交每个commits，大文件的commit会被替换为xxx.remove的flag文件）","pubDate":"Tue, 24 Nov 2020 03:51:17 GMT","guid":"http://yikun.github.io/2020/11/24/大文件在Github和Gitee上传的建议/","category":"Git,OpenSource"},{"title":"从数据压缩切入看MapReduce的全流程","link":"http://yikun.github.io/2020/08/20/从数据压缩切入看MapReduce的全流程/","description":"最近要在Hadoop中做一些和压缩库相关的优化，也借此机会把Hadoop的MapReduce的全流程代码进行了梳理，本篇文章将端到端的梳理一下MapReduce的全流程，并重点关注其中和数据压缩/解压相关的流程。 1. 起点为了使整个代码的阅读变得有趣，我们先提几个问题作为后续追寻蛛丝马迹的“导火索”： 算法支持情况。 Hadoop中，目前支持哪几种数据压缩算法？每个压缩算法有什么特点？性能如何？ 压缩框架实现。 Hadoop中的压缩算法的框架是怎样的？如何增加一个压缩算法？ 压缩与解压的时机。 在MapReduce的过程中，哪个过程会进行数据压缩和解压？耗时比例大概多少？会带来怎样的利弊？ 压缩性能测试。 如何进行压缩性能测试？ 2. MapReduce的基本流程 从上图我们可以看到MapReduce的核心流程如上所示，从用户的Input文件到最终的Output文件，主要经历以下几个阶段： Map阶段。Split：会将用户的输入文件，进行一些“分割“，在client端进行，逻辑上进行分割，只记录偏移信息。Map：Split文件信息会在Map阶段进行处理， 调用用户自己定义的Map函数。环形缓冲区：Map的输出不会直接存在文件里，而是存在环形缓冲区中，攒够了以后再进行落盘。Spill：从缓冲区落盘的过程叫做spill，也最终会生成多个Spill文件。Map.out：Spill文件最终会被合并为最终的Map输出。 Shuffle阶段。Shuffle阶段会将Map的输出下载到对应的Reduce的机器上。 Reduce阶段。Merge：Reduce阶段最开始的时候，会将Map文件进行Merge，形成一个大文件，作为Reduce的输入。Reduce：Reduce会执行用户自己定义的reduce函数，完成最终的输出。 更多详细的内容可以参考《Hadoop MapReduce Comprehensive Description》 [1] 这篇文章。 3. Hadoop中压缩算法的支持情况目前Hadoop支持的压缩算法共有2大类，一种是可分割的压缩算法，一种是不可分割的压缩算法。而支持的压缩算法的类型有： Lz4, 压缩速率很快，压缩比一般 Snappy, 由Google开源的压缩算法，压缩速率和压缩率均衡 ZSTD，由Facebook开源的压缩算法，压缩速率和压缩率均衡 Gzip/Zlib，GNU开源，压缩率不错，速度较慢 Passthrough，直通，仅保存压缩的文件名后缀，实际文件不压缩 Bzip2，压缩速率较慢，支持流式压缩，可分割 [1] Hadoop MapReduce Comprehensive Description: https://0x0fff.com/hadoop-mapreduce-comprehensive-description/ 4.后续在2020年9月26日，在Apache Hadoop Meetup上，我也分享了更多的技术细节，以及我们实际的性能测试结果： 视频：赋能Arm大数据开源生态，华为的探索之路：https://www.bilibili.com/video/av287309386/ 总结：Meetup 回顾 | 初探Hadoop on Arm: https://mp.weixin.qq.com/s/JgbHEqENHuJPZTPtwLZqqQ","pubDate":"Thu, 20 Aug 2020 13:14:16 GMT","guid":"http://yikun.github.io/2020/08/20/从数据压缩切入看MapReduce的全流程/","category":"大数据"},{"title":"搭建Hadoop Yarn环境 (ARM)","link":"http://yikun.github.io/2020/08/15/搭建Hadoop-Yarn环境-ARM/","description":"ARM上搭建Hadoop Yarn的指导。","pubDate":"Sat, 15 Aug 2020 00:00:00 GMT","guid":"http://yikun.github.io/2020/08/15/搭建Hadoop-Yarn环境-ARM/","category":"大数据,ARM"},{"title":"让压缩库ZSTD在ARM上更顺滑","link":"http://yikun.github.io/2020/05/20/让压缩库ZSTD在ARM上更顺滑/","description":"Facebook的ZSTD压缩库从1.0版本发布的那天起，就引起了业界的关注，对比业界常用的压缩库lz4、zilib、xz，ZSTD更注重速度和压缩比的均衡，对比zlib来看，更是在保证压缩比的情况下，较zlib压缩性能提升6倍左右，解压性能提升2倍左右。 我们团队也在2020年年初时，对ZSTD压缩库进行了性能优化，最终优化已推入到Facebook的上游社区中，本文将详细的介绍我们进行的优化。","pubDate":"Wed, 20 May 2020 00:00:00 GMT","guid":"http://yikun.github.io/2020/05/20/让压缩库ZSTD在ARM上更顺滑/","category":"OpenSource,ARM"},{"title":"让Github Action在你自己的机器上跑起来","link":"http://yikun.github.io/2020/04/17/让Github-Action在你自己的机器上跑起来/","description":"Github在2019年8月，宣布推出了一项新的功能——Github Action，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供Self Hosted Runners功能，让开发者们可以将自己的机器接入到Github中来。 最近，我们利用这一功能，将搭载着openEuler 20.03 (LTS) 操作系统，跑在Kunpeng 920 处理器的ARM环境接入进来，在近期华为与阿里合作的MPAM项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。 本篇文章将接入方法分享给大家，希望能够帮助更多同学们把自己的ARM环境也在Github上用起来。","pubDate":"Fri, 17 Apr 2020 09:03:23 GMT","guid":"http://yikun.github.io/2020/04/17/让Github-Action在你自己的机器上跑起来/","category":"Git"},{"title":"从Java Math底层实现看Arm与x86的差异","link":"http://yikun.github.io/2020/04/10/从Java-Math底层实现看Arm与x86的差异/","description":"1. 起初最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例： 123456public class Hello &#123; public static void main(String[] args) &#123; System.out.println(\"Math.log(3): \" + Math.log(3)); System.out.println(\"StrictMath.log(3): \" + StrictMath.log(3)); &#125;&#125;","pubDate":"Fri, 10 Apr 2020 00:58:09 GMT","guid":"http://yikun.github.io/2020/04/10/从Java-Math底层实现看Arm与x86的差异/","category":"Java"},{"title":"Github Action入门指南","link":"http://yikun.github.io/2020/02/28/Github-Action入门指南/","description":"这里记录一下Github Action的从入门到还没放弃的历程。 ：）","pubDate":"Fri, 28 Feb 2020 06:43:46 GMT","guid":"http://yikun.github.io/2020/02/28/Github-Action入门指南/","category":"Git"},{"title":"巧用Github Action同步代码到Gitee","link":"http://yikun.github.io/2020/01/17/巧用Github-Action同步代码到Gitee/","description":"1. 背景在开源贡献的代码托管的过程中，我们有时候有需要将Github的代码同步到其他远端仓库的需求。具体的，对于我们目前参与的项目来说核心诉求是：以Github社区作为主仓，并且定期自动同步到Gitee作为镜像仓库。 2. 调研 结论1: 由于会被Github屏蔽，Gitee的自动同步功能暂时无法支持。这个问题在Gitee的官方反馈中，建议github导入的项目能设置定时同步提及过，官方的明确答复是不支持。最近又再次和官方渠道求证，由于会被Github屏蔽的关系，这个功能不会被支持。本着有轮子用轮子，没轮子造轮子的原则，我们只能选择自己实现。 结论2: 靠手动同步存在时效问题，可能会造成部分commit的丢失。Gitee本身是提供了手动同步功能的，也算比较好用，但是想想看，如果一个组织下面，发展到有几百上千个项目后，这种机制显然无法解决问题了。因此，我们需要某种计算资源去自动的完成同步。 结论3: 目前我们开源的好几个项目（例如Mindspore, OpenGauss, Kunpeng）都有类似的需求。作为一个合格的程序员，为了守住DRY(don’t repeat yourself，不造重复的轮子)的原则，所以，我们需要实现一个工具，同步简单的配置就可以完成多个项目的同步。 最终结论：我们需要自己实现一个工具，通过某种计算资源自动的去完成周期同步功能。","pubDate":"Fri, 17 Jan 2020 09:26:56 GMT","guid":"http://yikun.github.io/2020/01/17/巧用Github-Action同步代码到Gitee/","category":"Git"},{"title":"源于鲲鹏，回归社区：GNU Glibc的ARM优化小记","link":"http://yikun.github.io/2019/12/30/源于鲲鹏，回归社区：GNU-Glibc的ARM优化小记/","description":"从2019年10月初开始，我们团队开始着手Glibc在aarch64(64)架构下的优化工作，并且在2019年年底，将我们的全部优化贡献给上游开源社区。本文分享我们在Glibc的版本完成的优化以及性能测试结果，同时我们也尝试着将优化的思路进行总结，希望对其他项目的优化提供一些思路。","pubDate":"Mon, 30 Dec 2019 00:00:00 GMT","guid":"http://yikun.github.io/2019/12/30/源于鲲鹏，回归社区：GNU-Glibc的ARM优化小记/","category":"ARM"},{"title":"OpenLab快速使用指南","link":"http://yikun.github.io/2019/04/12/OpenLab快速使用指南/","description":"希望本篇文章帮助你快速地了解如何快速地上手OpenLab。 1. Hello OpenLabOpenLab提供的核心能力之一，就是可以为项目提供计算资源（目前以虚拟机为主），用户可以通过配置来完成任务的定义，那么，先以”Hello OpenLab”为例介绍如何使用OpenLab的资源。","pubDate":"Fri, 12 Apr 2019 07:06:40 GMT","guid":"http://yikun.github.io/2019/04/12/OpenLab快速使用指南/","category":"OpenStack"},{"title":"[Placement深度探索] 共享、嵌套、分组模型深度解析","link":"http://yikun.github.io/2018/08/01/Placement深度探索-共享、嵌套、分组模型深度解析/","description":"在 #63 中我们介绍了最简单的Allocation candidate的过程，在Placment中，是如何实现分享、嵌套、分组的呢？ 1. 模型概览 如上图所示，对于一共三个节点，然后还有一个128G的共享内存： 节点1，含有16个VCPU、32768MB内存、包含2个NUMA分别挂2个PF，一个PF含8个VF，一个PF含2个VF 节点2，含有16个VCPU、32768MB内存 节点3，含有16个VCPU、16384MB内存 1.1 Single，独立模型。最简单的模型就是将每个Resource Rrovider(RP)看做独立的，当单个模型含有全部请求资源时，才算满足要求。即请求16个VCPU、16384MB的内存，那我们期待的就是获取到ID为1，6，7的节点。 1.2 Nested，嵌套模型。对于嵌套模型，我们期望的是，将整个树的关系都能被发现到，当一颗树上的资源满足请求，即返回树。即请求16个VCPU、32768MB的内存、8个VF，那么我们最终得到的是1节点，他是作为满足条件树的根节点。 1.3 Sharing，共享模型。对于共享模型，我们期望的是，将共享的NFS的资源也考虑进去，当然，这个资源仅共享给一个agg分组的RP，即请求16个VCPU、32768MB的内存、128G的硬盘，我们期望得到的是1节点和6节点。 1.4 Aggregate，分组模型。用户可以通过member_of指定agg，获取某个组内的资源。例如，我指定member_of agg1，那么我们期望得到的就是1节点和7节点。另外，还有一个场景，就是共享模型的提到的，即如果一个RP是共享的，那么在一个Aggregate中的RP都可以共享他的资源。","pubDate":"Wed, 01 Aug 2018 04:15:23 GMT","guid":"http://yikun.github.io/2018/08/01/Placement深度探索-共享、嵌套、分组模型深度解析/","category":"OpenStack"},{"title":"Python3内建函数扫盲","link":"http://yikun.github.io/2018/06/15/Python3内建函数扫盲/","description":"已完成11/68: 0. abs绝对值1. all（all items are True?） 迭代中所有元素均为True或者为空也返回true123456&gt;&gt;&gt; all([1,2,3])True&gt;&gt;&gt; all([1,2,3,0])False&gt;&gt;&gt; all([])True","pubDate":"Fri, 15 Jun 2018 02:40:26 GMT","guid":"http://yikun.github.io/2018/06/15/Python3内建函数扫盲/","category":"Python"},{"title":"Cell v2近期相关改进整理","link":"http://yikun.github.io/2018/06/06/Cell-v2近期相关改进整理/","description":"Handling a down cellhttps://review.openstack.org/#/c/557369在某个cell挂掉的时候，会影响跨Cell的查询、计算Quota等操作。在这个BP中提到了几个场景： nova list在一个Cell挂的时候，也需要能正常工作。在当前租户Cell没挂时，没影响，挂掉的话，需要构造一些数据（从api拿一些，然后剩下信息填UNKNOW） nova service-list在一个Cell挂的时候，也需要能正常工作。也是通过构造解决。 nova boot，短期解决方案是如果project有其他虚拟机在挂掉的cell不能创建虚拟机，长期解决方案是通过Placement计算quota和usage。","pubDate":"Wed, 06 Jun 2018 08:49:46 GMT","guid":"http://yikun.github.io/2018/06/06/Cell-v2近期相关改进整理/","category":"Nova,OpenStack"},{"title":"oslo.db中的死锁重试机制优化","link":"http://yikun.github.io/2018/04/19/oslo-db中的死锁重试机制优化/","description":"0. 引言OpenStack oslo.db是OpenStack中处理DB相关功能的基础组件，基本OpenStack所有的核心组件都会使用这一基础库。 去年12月，遇到一个死锁的问题 #62 一个死锁问题的深入探究，研究了下oslo.db死锁重试的方式，发现其中并没有加入随机机制。在通信领域，有一个叫做“二进制退避机制”的算法（嗯，也算没白读了7年通信，哈哈，在本文立刻提升逼格），就是通过指数递增+随机的方式来解决无中心、多节点接入时产生的冲突的。 当时，顺着这个思路，我在oslo.db中提交了一个关于改进死锁重试机制的[patch/527362]：Improve exponential backoff for wrap_db_retry。4个多月后，终于合入了。写这篇文章主要是为了记录一下自己学习的过程，以及对死锁及其重试机制的思考。","pubDate":"Thu, 19 Apr 2018 02:26:46 GMT","guid":"http://yikun.github.io/2018/04/19/oslo-db中的死锁重试机制优化/","category":"OpenStack,数据库"},{"title":"[Placement深度探索] Granular Resource Request Syntax","link":"http://yikun.github.io/2018/04/02/Placement深度探索-Granular-Resource-Request-Syntax/","description":"1. 问题背景在进行资源请求的时候，由于目前支持的能力有限，我们目前只能请求一个单一类型包含整数数量的资源。 例如，我们请求VCPU为2，内存为2G，要求其架构为X86架构，即通过以下URL进行请求： GET /allocation_candidates?resources=VCPU:2,MEMORY_MB:2048&amp;required=HW_CPU_X86_AVX 也不能指定我们需要某一个RP具有某种特质，所有不同类型的资源也只能从一个RP提供。","pubDate":"Mon, 02 Apr 2018 02:54:51 GMT","guid":"http://yikun.github.io/2018/04/02/Placement深度探索-Granular-Resource-Request-Syntax/","category":"Nova,OpenStack"},{"title":"Nova Scheduler Team Meeting跟踪（三月）","link":"http://yikun.github.io/2018/03/31/Nova-Scheduler-Team-Meeting跟踪（三月）/","description":"2018年3月5日PTG刚开完，没什么太多的事儿，jaypipes说了几点： jaypipes会发一个recap总结下R版本的重点http://lists.openstack.org/pipermail/openstack-dev/2018-March/128041.html 最开始的3-4周，都会集中在update_provider_tree系列的patch落地 在Resource tracker刷新额外traits的合并问题需要讨论，可以在update-provider-tree完成之后去做 2018年3月5日Feature讨论 Support traits in Glance https://review.openstack.org/#/c/541507/ Add placement-req-filter spec https://review.openstack.org/#/c/544585/这是调度流程的一个很大变化，这个BP源自CERN从v1升v2的一个需求，最开始CERN用的是Cell v1规模挺大，大概有上万个计算节点，原来的用法是：第一级调度：一个租户映射到指定的特定Cell中，一般一个Cell中也会把特殊硬件的计算节点集中起来，第二级调度：这样在Cell v1中通过租户找到的Cell，然后剩余的节点就不多了，然后进行第二级调度，调度在Cell内压力就小多了。但是目前Placement是一个全局的，并不能感知到Cell，也就是说最差情况，Placement过滤得不好，可能导致真正Scheduler的时候，有上万的节点，所以，就在Placment前面加了一个步骤Pre filter。目前的作用就是，把Placement做不到的，自定义程度很高的Filter放到这里来。 Forbidden Traits https://review.openstack.org/#/c/548915/required里面通过叹号来表示不想要某种traits Support default allocation ratios https://review.openstack.org/#/c/552105/","pubDate":"Sat, 31 Mar 2018 10:26:13 GMT","guid":"http://yikun.github.io/2018/03/31/Nova-Scheduler-Team-Meeting跟踪（三月）/","category":"Nova,OpenStack"},{"title":"Nova Scheduler Team Meeting跟踪（二月）","link":"http://yikun.github.io/2018/02/07/Nova-Scheduler-Team-Meeting跟踪（二月）/","description":"1. 会议记录2018年2月5日(1) Feature讨论目前已经Freature Freeze了，因此，对于BP来说，没有什么太多更新了，只是简单的罗列了下相关的Patch。Provider Tree series starting with: https://review.openstack.org/#/c/537648/Nested RP traits selection: https://review.openstack.org/#/c/531899/目前，Nested RP的这2部分工作也不会在Queens版本合入了，会推迟到Rocky。 Granular resource requests review: https://review.openstack.org/#/c/517757/resource和requeired分组的支持，API部分的PatchQueens版本未完成。 Remove microversion fallback:https://review.openstack.org/#/c/528794/由于目前Queens已经使用1.14作为默认的microversion，因此，对于之前的一些兼容版本不会再使用了，所以对之前的兼容代码进行了清理。 Use alternate hosts for resize:https://review.openstack.org/#/c/537614/Alternate hosts已合入，上面是补了一些test case (2) Bug讨论1. Generation及重试问题Add generation support in aggregate association https://review.openstack.org/#/c/540447/没有什么新的bug了，在之前讨论的aggregate相关的API增加generation的问题，cdent提了一个BP，会在Rocky版本完成。placement server needs to retry allocations, server-side https://bugs.launchpad.net/nova/+bug/1719933对于并发更新时的重试问题，还是有一些讨论，@edleafe 认为，对于一些场景，请求aloocation时，用户认为有足够容量呀，不能够失败。@jaypipes还是原来的意见： it should “fail” in so much as a 409 Conflict is returned and allows the caller to retry if it wants. 也就是说，409肯定是要失败，重试的事情需要调用他的人来做。当然，也会在PTG讨论下，generation到底怎么样去使用和暴露。已经把这个问题记到nova-ptg-rocky：Do we have a concurrency problem with PUT /allocations/{consumer_uuid} and/or POST /allocations ? (3) 开放讨论Placement queens summary https://anticdent.org/placement-queens-summary.htmlPlacement extraction https://anticdent.org/placement-extraction.html 关于将Placement抽离@cdent 完成了两篇文章，一个是queens版本的placement总结，另外一个是cdent做的，关于将Placement从Nova抽离出来的一些工作。关于将Placement抽离出来，大家发表了自己的看法：@cdent 他认为，较早的把Placement分离出来，对于Placement和Nova来说都好，目前抽离的工作量比较小，好分离，另外，目前Nova投入的大量的时间和优先级放在Placement相关的事务上，分离出来，对Nova好一些。@bauzas 不太同意现在去分离，他主要是担心Nova和Placement分离后，有点难协调。 2018年2月12日(1) Feature讨论目前的Feature的讨论，已经开始Rocky版本的了。 1. Support traits in Glancehttps://review.openstack.org/#/c/541507/这个BP主要是希望为Glance增加Traits支持，在Glance的Properties中，增加类似”trait:HW_CPU_X86_AVX2=required”, “trait:CUSTOM_TRUSTED_HOST=required”的支持，让Placement调度的时候支持。 2. Resource Class Affinity Spechttps://review.openstack.org/543062efried写的一个bp，看名字知其意，调度的时候考虑Resouce Class的亲和。 (2) bug讨论Handle volume-backed instances in IsolatedHostsFilter：https://review.openstack.org/#/q/topic:bug/1746483+(status:open+OR+status:merged)Matt发现了一个Filter的问题，主要是对volume-backed的情况进行一些异常处理。在Scheduler会议中，已经很久没有讨论过非Placement的问题。- -! (3) 开放讨论Add optional healthcheck middleware https://review.openstack.org/#/c/542992/一个用于健康检查的midleware，对于API服务挺有用，尤其是对于LB场景下的检查活跃来说。Feature的spec在这里：https://review.openstack.org/#/c/531456/ 2018年2月19日(1) Feature讨论Glance image traits https://review.openstack.org/#/c/541507/Resource class的亲和性 至少到S版本才会落（包括在Placement中支持NUMA亲和），优先级不高，提了下Placement RBAC的需求(Policy/RBAC support in Placement REST API)可能会更高一些。update provider tree的优先级很高解决了很多问题 (2) Bug讨论Placement returns 503 when Keystone is down https://bugs.launchpad.net/nova/+bug/1749797Keystone挂的时候，Placement会返回一个503，这个问题最后是在keystone middleware里面加了一些detail信息: https://review.openstack.org/546108 (3)开放讨论调度失败的”Nova valid host”足够了吗？@arvindn05 这哥们提到在虚拟机调度的时候，我们仅仅返回了”no valid host”，为啥不503一个，然后返回为啥调度失败。@edleafe 说了2点，503肯定不合适，错误是用户，不是系统。详细信息不显示是因为不想把底层的硬件架构拓扑之类的信息暴露给用户。管理员可以通过日志之类的看到失败原因。","pubDate":"Wed, 07 Feb 2018 18:40:58 GMT","guid":"http://yikun.github.io/2018/02/07/Nova-Scheduler-Team-Meeting跟踪（二月）/","category":"Nova,OpenStack"},{"title":"Nova Scheduler Team Meeting跟踪（一月）","link":"http://yikun.github.io/2018/02/01/Nova-Scheduler-Team-Meeting跟踪（一月）/","description":"从今年开始，要细度每次的Nova Meeting了，确实对于整体把握整体社区某个领域的进度非常有用。我是这样设想的，按月汇总，每次一篇文章，包含以下几部分 记录。按照meeting日期，记录主要内容 总结。总结每次meeting的每次内容，简短的一句话或者一段话，避免流水账 TODO。每次meeting不一定能完全理解，把他们记录下来，学习后闭环。 1. 会议记录2018年1月8日2018年的第一个team meeting，我们可以看到重点的工作还是在Nested Resource Provider这个BP，在这个时间，大家还是希望能够把Nested Resource Provider这个BP在Queens版本完成。 (1) Feature讨论1. Nested Resource Provider @2uasimojo(efried) 正在完成ComputeDriver.update_provider_tree() https://review.openstack.org/#/c/521685/ @jaypipes 正在完成GET /allocation_candidates部分 https://review.openstack.org/#/c/531443/@bauzas 表示Xen可能是Nested Resource Provider最棒的目标用户。 @jaypipes 目前NRP的目标还是Queens版本完成，可以把NRP的report部分、candidates部分、xen作为client/consumer在Queens完成 而NUMA/PCI部分的工作，估计搞不定，所以意味着我们在Queens还是需要PciPassthroughFilter及NUMATopologyFilter driver consumption的工作会在Queens完成，包括driver通过update_provider_tree来上报信息给RP，也包括了从scheduler中基于allocation来做设备的创建和分配。 2. Alternate hosts解决了一个bug/1741125 Instance resize intermittently fails when reschedulinghttps://review.openstack.org/#/c/531022/ 3. limit on allocation_candidatesdansmith增加了一个CONF.scheduler.max_placement_results，用于限制每次备选节点的请求，默认1000https://review.openstack.org/#/c/531517/ (2) Open discussion关于Resource Provider的genration id的讨论。在随后的开放讨论中，由于Resource Provider的aggregate信息在更新时，会有在不同节点上的多个请求并发进行更新的问题，我们需要一种方案去解决race conditions。是的，就是我们在 #65 提到的方法。 @2uasimojo(efried) 提到，这种方案并不是进程或者线程的锁，建议按照原来的实现，给更新RP的aggregate加上genration id，用于解决并发下的竞态更新问题。即在PUT的时候，用户需要传入genration id，这个id就是Get时候的genration id。这种方案看似有点土，我更新个字段还得自己传genration，太不方便了。但是，却是一种很好的方法来解决从Get直到PUT入库中间的竞争。大家对这点，达成了一致，另外，我们在更新rp的aggregate的时候，仅更新正更新的rp的generation，而不需要更新aggregate中其他rp的genration。 最终，决定让 @cdent 去做generations-on-aggregate placement microversion相关的patch。 关于conflict 409后重试机制的讨论@2uasimojo(efried) 提出了这个问题，对于409的处理，一直不是很清晰，因为我们重试的时候，不知道到底应该是仅仅重试之前的操作，还是说再看看这个数据是不是已经更新之类的。 @jaypipes 说，发生409后，更新的调用者，需要回答一个问题“OK，我们需要更新的东西已经变了，在我进行重试时，检查一下我想要更新的东西是否已经更新过了”，所有的generation变化，只是表达了“something changed”，而不是“this thins changed”。所以在我们进行409的重试时，我们需要重读下所有的provider信息（比如traits、inventory等），然后检查下，我们想更新的东西是否已经存在了，如果是这样的话，我们什么都不做，如果没有，我们需要重新的调用update/set。这个想要更新的状态取决于virt driver，和他希望做什么。（比如更新inventory和traits肯定是不一样的）。总结来说，就是我们最初的设计：client-driven state retries，而不是傻傻的重试。 本次Meeting总的来说还是充满干货的，尤其是对generation和409重试的讨论。 2018年1月15日(1) Feature讨论1. Nested Resource ProvidersNRP的进度没有太大进展，目前包含update_provider_tree和GET /allocation_candidates两部分内容。 2. Granular resource requests这个是为了支持用户进行复杂资源请求的bp，最近会专门写一个文章记录一下其实现。 3. Alternate Hosts目前这个特性基本完成了，相关Patch：patch/526436 Change compute RPC to use alternates for resize (2) Bug讨论bug/1743120: placement inadvertently imports many python modules it does not need这个bug主要是说Placement导入了很多不需要的模块，主要是和Nova耦合太近，不利于后面拆分，并且直接使用Nova的也不够简洁。所以，清理、化简，保持干净。 Open discussionProviderTree accessorsPatch在这里：https://review.openstack.org/#/c/533244主要为了对比ComputeDriver.update_provider_tree和缓存在report client的ProviderTree的变化。抽象出来了一个结构ProviderData，专门来返回数据。 总的来说，本次Meeting的讨论内容较少，集中在Nested Resource Provider上面。 2018年1月22日重要事件：1月25日，Queens版本的Feature Freeze即将到来。 (1) Feature讨论1. Nested Resource Providers目前还是包括update_provider_tree series和Nested RP selection两部分。update_provider_tree series接近完成了（不包括resource tracker端到端的上报），Nested RP selection，会推到Rocky版本。 2. Request Traits in NovaNova中支持请求traits，另外这个请求也额外的提到了Granular resource requests特性，有部分功能是重合的，后续分析Granular resource requests时候，重点关注下。 3. Use alternate hosts for resizeAlternate hosts这个bp已经基本完成，后续也需要学习下。 (2) Bug讨论Remove microversion fallback code from report clienthttps://review.openstack.org/#/c/528794/ 在Queens版本，nova默认支持1.14了，所以移除了一些之前版本的兼容代码。 2018年1月29日(1) Feature讨论1. Nested Resource ProvidersProvider Tree series部分的工作已完成，https://review.openstack.org/#/c/533808/ First provider tree patch in progress： https://review.openstack.org/#/c/537648/ 这部分是端到端的从resource tracker中调用driver的update tree，应该会推到Rocky去做Nested RP traits selection: https://review.openstack.org/#/c/531899/ 没有什么进展从开放讨论中，@efried 提到，想要端到端的使用NRP，需要完成三部分：a. Resource Tracker刷新update_provider_tree b. jaypieps的NRP in alloc cands c. driver实现update_provider_tree。这三项工作，都没有在Queens完成，不过都比较接近完成了。 2. Singular request group traits基本完成 3. Granular resource requests完整实现推迟到Queens版本，https://review.openstack.org/#/c/517757/ 4. Use alternate hosts for resizehttps://review.openstack.org/#/c/537614/ 已经merge，至此，已经可以支持resize时候的alternate hostsl了 (2) 开放讨论1. Idea for a simple way to expose compute driver capabilities in the REST APIhttp://lists.openstack.org/pipermail/openstack-dev/2018-January/126653.html Matt提出希望用一种简单方法保持driver的兼容 2. TODO 了解Idea for a simple way to expose compute driver capabilities in the REST API详细内容 Granular resource requests分析 Alternate hosts分析 Nested Resource Provider分析","pubDate":"Thu, 01 Feb 2018 20:32:51 GMT","guid":"http://yikun.github.io/2018/02/01/Nova-Scheduler-Team-Meeting跟踪（一月）/","category":"Nova,OpenStack"},{"title":"Nova Scheduler Team Meeting跟踪","link":"http://yikun.github.io/2018/02/01/Nova-Scheduler-Team-Meeting跟踪/","description":"从今年开始，要细度每次的Nova Meeting了，确实对于整体把握整体社区某个领域的进度非常有用。我是这样设想的，按月汇总，每次一篇文章，包含以下几部分 记录。按照meeting日期，记录主要内容 总结。总结每次meeting的每次内容，简短的一句话或者一段话，避免流水账 TODO。每次meeting不一定能完全理解，把他们记录下来，学习后闭环。 1. 会议记录2018年1月8日2018年的第一个team meeting，我们可以看到重点的工作还是在Nested Resource Provider这个BP，在这个时间，大家还是希望能够把Nested Resource Provider这个BP在Queens版本完成。 (1) Feature讨论1. Nested Resource Provider @2uasimojo(efried) 正在完成ComputeDriver.update_provider_tree() https://review.openstack.org/#/c/521685/ @jaypipes 正在完成GET /allocation_candidates部分 https://review.openstack.org/#/c/531443/@bauzas 表示Xen可能是Nested Resource Provider最棒的目标用户。 @jaypipes 目前NRP的目标还是Queens版本完成，可以把NRP的report部分、candidates部分、xen作为client/consumer在Queens完成 而NUMA/PCI部分的工作，估计搞不定，所以意味着我们在Queens还是需要PciPassthroughFilter及NUMATopologyFilter driver consumption的工作会在Queens完成，包括driver通过update_provider_tree来上报信息给RP，也包括了从scheduler中基于allocation来做设备的创建和分配。 2. Alternate hosts解决了一个bug/1741125 Instance resize intermittently fails when reschedulinghttps://review.openstack.org/#/c/531022/ 3. limit on allocation_candidatesdansmith增加了一个CONF.scheduler.max_placement_results，用于限制每次备选节点的请求，默认1000https://review.openstack.org/#/c/531517/ (2) Open discussion关于Resource Provider的genration id的讨论。在随后的开放讨论中，由于Resource Provider的aggregate信息在更新时，会有在不同节点上的多个请求并发进行更新的问题，我们需要一种方案去解决race conditions。是的，就是我们在 #65 提到的方法。 @2uasimojo(efried) 提到，这种方案并不是进程或者线程的锁，建议按照原来的实现，给更新RP的aggregate加上genration id，用于解决并发下的竞态更新问题。即在PUT的时候，用户需要传入genration id，这个id就是Get时候的genration id。这种方案看似有点土，我更新个字段还得自己传genration，太不方便了。但是，却是一种很好的方法来解决从Get直到PUT入库中间的竞争。大家对这点，达成了一致，另外，我们在更新rp的aggregate的时候，仅更新正更新的rp的generation，而不需要更新aggregate中其他rp的genration。 最终，决定让 @cdent 去做generations-on-aggregate placement microversion相关的patch。 关于conflict 409后重试机制的讨论@2uasimojo(efried) 提出了这个问题，对于409的处理，一直不是很清晰，因为我们重试的时候，不知道到底应该是仅仅重试之前的操作，还是说再看看这个数据是不是已经更新之类的。 @jaypipes 说，发生409后，更新的调用者，需要回答一个问题“OK，我们需要更新的东西已经变了，在我进行重试时，检查一下我想要更新的东西是否已经更新过了”，所有的generation变化，只是表达了“something changed”，而不是“this thins changed”。所以在我们进行409的重试时，我们需要重读下所有的provider信息（比如traits、inventory等），然后检查下，我们想更新的东西是否已经存在了，如果是这样的话，我们什么都不做，如果没有，我们需要重新的调用update/set。这个想要更新的状态取决于virt driver，和他希望做什么。（比如更新inventory和traits肯定是不一样的）。总结来说，就是我们最初的设计：client-driven state retries，而不是傻傻的重试。 本次Meeting总的来说还是充满干货的，尤其是对generation和409重试的讨论。 2018年1月15日(1) Feature讨论1. Nested Resource ProvidersNRP的进度没有太大进展，目前包含update_provider_tree和GET /allocation_candidates两部分内容。 2. Granular resource requests这个是为了支持用户进行复杂资源请求的bp，最近会专门写一个文章记录一下其实现。 3. Alternate Hosts目前这个特性基本完成了，相关Patch：patch/526436 Change compute RPC to use alternates for resize (2) Bug讨论bug/1743120: placement inadvertently imports many python modules it does not need这个bug主要是说Placement导入了很多不需要的模块，主要是和Nova耦合太近，不利于后面拆分，并且直接使用Nova的也不够简洁。所以，清理、化简，保持干净。 Open discussionProviderTree accessorsPatch在这里：https://review.openstack.org/#/c/533244主要为了对比ComputeDriver.update_provider_tree和缓存在report client的ProviderTree的变化。抽象出来了一个结构ProviderData，专门来返回数据。 总的来说，本次Meeting的讨论内容较少，集中在Nested Resource Provider上面。 2018年1月22日重要事件：1月25日，Queens版本的Feature Freeze即将到来。 (1) Feature讨论1. Nested Resource Providers目前还是包括update_provider_tree series和Nested RP selection两部分。update_provider_tree series接近完成了（不包括resource tracker端到端的上报），Nested RP selection，会推到Rocky版本。 2. Request Traits in NovaNova中支持请求traits，另外这个请求也额外的提到了Granular resource requests特性，有部分功能是重合的，后续分析Granular resource requests时候，重点关注下。 3. Use alternate hosts for resizeAlternate hosts这个bp已经基本完成，后续也需要学习下。 (2) Bug讨论Remove microversion fallback code from report clienthttps://review.openstack.org/#/c/528794/ 在Queens版本，nova默认支持1.14了，所以移除了一些之前版本的兼容代码。 2018年1月29日(1) Feature讨论1. Nested Resource ProvidersProvider Tree series部分的工作已完成，https://review.openstack.org/#/c/533808/ First provider tree patch in progress： https://review.openstack.org/#/c/537648/ 这部分是端到端的从resource tracker中调用driver的update tree，应该会推到Rocky去做Nested RP traits selection: https://review.openstack.org/#/c/531899/ 没有什么进展从开放讨论中，@efried 提到，想要端到端的使用NRP，需要完成三部分：a. Resource Tracker刷新update_provider_tree b. jaypieps的NRP in alloc cands c. driver实现update_provider_tree。这三项工作，都没有在Queens完成，不过都比较接近完成了。 2. Singular request group traits基本完成 3. Granular resource requests完整实现推迟到Queens版本，https://review.openstack.org/#/c/517757/ 4. Use alternate hosts for resizehttps://review.openstack.org/#/c/537614/ 已经merge，至此，已经可以支持resize时候的alternate hostsl了 (2) 开放讨论1. Idea for a simple way to expose compute driver capabilities in the REST APIhttp://lists.openstack.org/pipermail/openstack-dev/2018-January/126653.html Matt提出希望用一种简单方法保持driver的兼容 1. 会议记录2018年2月5日(1) Feature讨论目前已经Freature Freeze了，因此，对于BP来说，没有什么太多更新了，只是简单的罗列了下相关的Patch。Provider Tree series starting with: https://review.openstack.org/#/c/537648/Nested RP traits selection: https://review.openstack.org/#/c/531899/目前，Nested RP的这2部分工作也不会在Queens版本合入了，会推迟到Rocky。 Granular resource requests review: https://review.openstack.org/#/c/517757/resource和requeired分组的支持，API部分的PatchQueens版本未完成。 Remove microversion fallback:https://review.openstack.org/#/c/528794/由于目前Queens已经使用1.14作为默认的microversion，因此，对于之前的一些兼容版本不会再使用了，所以对之前的兼容代码进行了清理。 Use alternate hosts for resize:https://review.openstack.org/#/c/537614/Alternate hosts已合入，上面是补了一些test case (2) Bug讨论1. Generation及重试问题Add generation support in aggregate association https://review.openstack.org/#/c/540447/没有什么新的bug了，在之前讨论的aggregate相关的API增加generation的问题，cdent提了一个BP，会在Rocky版本完成。placement server needs to retry allocations, server-side https://bugs.launchpad.net/nova/+bug/1719933对于并发更新时的重试问题，还是有一些讨论，@edleafe 认为，对于一些场景，请求aloocation时，用户认为有足够容量呀，不能够失败。@jaypipes还是原来的意见： it should “fail” in so much as a 409 Conflict is returned and allows the caller to retry if it wants. 也就是说，409肯定是要失败，重试的事情需要调用他的人来做。当然，也会在PTG讨论下，generation到底怎么样去使用和暴露。已经把这个问题记到nova-ptg-rocky：Do we have a concurrency problem with PUT /allocations/{consumer_uuid} and/or POST /allocations ? (3) 开放讨论Placement queens summary https://anticdent.org/placement-queens-summary.htmlPlacement extraction https://anticdent.org/placement-extraction.html 关于将Placement抽离@cdent 完成了两篇文章，一个是queens版本的placement总结，另外一个是cdent做的，关于将Placement从Nova抽离出来的一些工作。关于将Placement抽离出来，大家发表了自己的看法：@cdent 他认为，较早的把Placement分离出来，对于Placement和Nova来说都好，目前抽离的工作量比较小，好分离，另外，目前Nova投入的大量的时间和优先级放在Placement相关的事务上，分离出来，对Nova好一些。@bauzas 不太同意现在去分离，他主要是担心Nova和Placement分离后，有点难协调。 2018年2月12日(1) Feature讨论目前的Feature的讨论，已经开始Rocky版本的了。 1. Support traits in Glancehttps://review.openstack.org/#/c/541507/这个BP主要是希望为Glance增加Traits支持，在Glance的Properties中，增加类似”trait:HW_CPU_X86_AVX2=required”, “trait:CUSTOM_TRUSTED_HOST=required”的支持，让Placement调度的时候支持。 2. Resource Class Affinity Spechttps://review.openstack.org/543062efried写的一个bp，看名字知其意，调度的时候考虑Resouce Class的亲和。 (2) bug讨论Handle volume-backed instances in IsolatedHostsFilter：https://review.openstack.org/#/q/topic:bug/1746483+(status:open+OR+status:merged)Matt发现了一个Filter的问题，主要是对volume-backed的情况进行一些异常处理。在Scheduler会议中，已经很久没有讨论过非Placement的问题。- -! (3) 开放讨论Add optional healthcheck middleware https://review.openstack.org/#/c/542992/一个用于健康检查的midleware，对于API服务挺有用，尤其是对于LB场景下的检查活跃来说。Feature的spec在这里：https://review.openstack.org/#/c/531456/ 2018年2月19日(1) Feature讨论Glance image traits https://review.openstack.org/#/c/541507/Resource class的亲和性 至少到S版本才会落（包括在Placement中支持NUMA亲和），优先级不高，提了下Placement RBAC的需求(Policy/RBAC support in Placement REST API)可能会更高一些。update provider tree的优先级很高解决了很多问题 (2) Bug讨论Placement returns 503 when Keystone is down https://bugs.launchpad.net/nova/+bug/1749797Keystone挂的时候，Placement会返回一个503，这个问题最后是在keystone middleware里面加了一些detail信息: https://review.openstack.org/546108 (3)开放讨论调度失败的”Nova valid host”足够了吗？@arvindn05 这哥们提到在虚拟机调度的时候，我们仅仅返回了”no valid host”，为啥不503一个，然后返回为啥调度失败。@edleafe 说了2点，503肯定不合适，错误是用户，不是系统。详细信息不显示是因为不想把底层的硬件架构拓扑之类的信息暴露给用户。管理员可以通过日志之类的看到失败原因。 2018年3月5日PTG刚开完，没什么太多的事儿，jaypipes说了几点： jaypipes会发一个recap总结下R版本的重点http://lists.openstack.org/pipermail/openstack-dev/2018-March/128041.html 最开始的3-4周，都会集中在update_provider_tree系列的patch落地 在Resource tracker刷新额外traits的合并问题需要讨论，可以在update-provider-tree完成之后去做 2018年3月5日Feature讨论 Support traits in Glance https://review.openstack.org/#/c/541507/ Add placement-req-filter spec https://review.openstack.org/#/c/544585/这是调度流程的一个很大变化，这个BP源自CERN从v1升v2的一个需求，最开始CERN用的是Cell v1规模挺大，大概有上万个计算节点，原来的用法是：第一级调度：一个租户映射到指定的特定Cell中，一般一个Cell中也会把特殊硬件的计算节点集中起来，第二级调度：这样在Cell v1中通过租户找到的Cell，然后剩余的节点就不多了，然后进行第二级调度，调度在Cell内压力就小多了。但是目前Placement是一个全局的，并不能感知到Cell，也就是说最差情况，Placement过滤得不好，可能导致真正Scheduler的时候，有上万的节点，所以，就在Placment前面加了一个步骤Pre filter。目前的作用就是，把Placement做不到的，自定义程度很高的Filter放到这里来。 Forbidden Traits https://review.openstack.org/#/c/548915/required里面通过叹号来表示不想要某种traits Support default allocation ratios https://review.openstack.org/#/c/552105/ 2. TODO 了解Idea for a simple way to expose compute driver capabilities in the REST API详细内容 Granular resource requests分析 Alternate hosts分析 Nested Resource Provider分析","pubDate":"Thu, 01 Feb 2018 12:32:51 GMT","guid":"http://yikun.github.io/2018/02/01/Nova-Scheduler-Team-Meeting跟踪/","category":"Nova,OpenStack"},{"title":"[Placement深度探索] Resource Provider中的并发控制机制","link":"http://yikun.github.io/2018/01/23/Placement深度探索-Resource-Provider中的并发控制机制/","description":"1. 背景最近，在处理Nova Metadata并发更新的问题(bug/1650188)的时候，发现Resource Provider的并发控制机制在最开始就考虑，是通过乐观锁的机制实现并发控制的，简单的说就是： 为Resource Provider增加了一个generation的字段，用来记录数据更新迭代的版本。 每次进行刷新（如新增、删除、更新）的时候，检查generation和最初读取的是否一致，若一致则字段值自增，完成数据更新，否则抛出并发更新的异常，返回给用户一个409。 其实，这个方式就是我们常说的乐观并发控制（OCC, Optimistic Concurrency Control，也称作乐观锁）机制。 2. 详细流程用户通过API对Resource Provider的资源进行更新时，会传入一个generation参数 curl -X PUT http://10.76.6.31/placement/resource_providers/7d2590ae-9999-4080-9306-058b4c915e32/traits -H “X-Auth-Token: $TOKEN” -H “OpenStack-API-Version: placement 1.16” -H “Accept: application/json” -H “Content-Type: application/json” -d ‘{ “resource_provider_generation”: 0, “traits”: [“CUSTOM_YIKUN_TEST”]}’ 在最终的数据刷新时，完成事务提交前，会对generation进行刷新，例如对于本例中的traits更新，对应的代码在这里：nova/objects/resource_provider.py#def _set_traits，相当于做了一次检查，如果generation和用户预期的一致，更新成功，如果更新失败，则会raise并发更新失败的error。 如上图所示，如果操作A和操作B并发的请求进来，当A请求成功后，刷新了genration，这样，当B进行刷新的时候，就会刷新失败。 在Placement中，在对Resource Provider下的资源（例如allocation、inventory、trait等）进行修改时，均会对resource provider的generation进行刷新。我们看下实现的细节：12345678910111213141516171819202122232425def _increment_provider_generation(ctx, rp): \"\"\"Increments the supplied provider's generation value, supplying the currently-known generation. Returns whether the increment succeeded. :param ctx: `nova.context.RequestContext` that contains an oslo_db Session :param rp: `ResourceProvider` whose generation should be updated. :returns: The new resource provider generation value if successful. :raises nova.exception.ConcurrentUpdateDetected: if another thread updated the same resource provider's view of its inventory or allocations in between the time when this object was originally read and the call to set the inventory. \"\"\" rp_gen = rp.generation new_generation = rp_gen + 1 # 注意这里的更新条件，通过id及generation匹配 upd_stmt = _RP_TBL.update().where(sa.and_( _RP_TBL.c.id == rp.id, _RP_TBL.c.generation == rp_gen)).values( generation=(new_generation)) res = ctx.session.execute(upd_stmt) # 如果rowcount为0，说明已经不是之前的RP了 if res.rowcount != 1: raise exception.ConcurrentUpdateDetected return new_generation 3. 参考 On Optimistic Methods for Concurrency Control：对乐观并发控制机制及其要点进行了一些总结。 阿里巴巴Java开发手册： 并发修改同一记录时,避免更新丢失,要么在应用层加锁,要么在缓存加锁,要么在数据库层使用乐观锁,使用 version 作为更新依据。 说明:如果每次访问冲突概率小于 20%,推荐使用乐观锁,否则使用悲观锁。乐观锁的重试次数不得小于 3 次。 深入理解乐观锁与悲观锁：介绍了乐观锁和悲观锁的基本原理，并举例说明。","pubDate":"Tue, 23 Jan 2018 02:02:42 GMT","guid":"http://yikun.github.io/2018/01/23/Placement深度探索-Resource-Provider中的并发控制机制/","category":"Nova,OpenStack"},{"title":"[Placement深度探索] Nested Resource Providers","link":"http://yikun.github.io/2017/12/26/Placement深度探索-Nested-Resource-Providers/","description":"1. 背景概述顾名思义，Nested Resource Providers，即嵌套的资源提供者。在Ocata版本，这个bp/nested-resource-providers就被提出，主要是为了使用户可以定义不同的Resource Provider之间的层级关系（hierarchical relationship）。 我们知道，目前Placement的功能已初具雏形，我们可以记录系统中可数的资源的总数。一个Resource Provider有一系列不同资源种类的存量信息（Inventory），也通过已分配量（Allocation）信息来记录已使用量。通过Resource Provider/Inventory/Allocation这三个关键模型，我们就可以解决以下几个需求： 每个Resource Provider有多少某种类型的资源？通过Invetory记录，例如某个主机VCPU的总量； 系统已经消耗了多少某种类型的资源？通过Allocation记录，例如某个虚拟机消耗了1GB的内存； 每个Resource Provider为某种类型的资源提供多少超分配的能力？通过Inventory的allocation_ratio字段来记录。 如下图所示，一个计算节点包含8个CPU，500GB硬盘，16GB内存，已使用3个CPU，3GB硬盘，2GB内存，这个计算节点所属高IO组，具备SSD的能力，抽象为Placement模型后，若下图所示： 计算节点对应Resource Provider（蓝色），其包含的某种类型资源的总量对应Inventory（紫色），资源的类型对应Resource Class（灰色），已使用量对应Allocation（绿色），所属的组对应Placement组（黄色），计算节点的特质对应Trait（橙色）。 在之前的实现中，对于RP之间的关系，也仅仅支持aggregate功能。例如，某个RP可以把自己的资源，通过aggregate将RP的资源共享给同一aggregate的其他RP。这一功能对于共享存储、共享IP池之类的业务是满足需求的，但是，对于类似父子的这种关系，是无法支持的。 例如，在NUMA场景下，我们不但需要将主机的内存和VCPU资源记录，同时也需要记录每个主机上的某个NUMA的资源总量及消耗情况，这个就属于父子关系。 2. 存在的问题在nested-resource-providers中提到一场景： there are resource classes that represent a consumable entity that is within another consumable entity. An example of such a resource class is the amount of memory “local” to a particular NUMA cell. 就是说一些类型代表一种资源消费的实体，同时，包含了另外一种资源消费的实体。举个例子就是memory这种resource class，这个memory是属于某个NUMA的。让我们通过一个例子来看下这个问题。 在一些对性能或时延有苛刻要求的场景，我们通常希望一个虚拟机能够部署到某个NUMA上（一个主机通常含有多个NUMA CELL，注意这个CELL和我们在Nova中说的Cell V2不是一个含义，而是NUMA独有的名词）。假设我们已经创建了一个叫做“NUMA_MEMORY_MB”的资源类型，资源的总量是192GB。当我们希望将它创建到一个磁盘大小充足并且NUMA内存充足的主机上时，如果仅考虑这个主机上的总内存，可能会找到一个并不是我们期望的主机。我们必须考虑每个NUMA CELL中的内存是否充足。 如上图所示，假设一个主机总共有192GB内存。其中128GB分配给了NUMA CELL0，另外64GB分配给了NUMA CELL1。 虚拟机A消耗了112GB内存，落在了NUMA CELL0上，NUMA CELL0还剩16GB内存 虚拟机B消耗了48GB内存，落在了NUMA CELL1上，NUMA CELL1还剩16GB内存 虚拟机C来的时候说：我需要32GB内存。我们应该如何调度主机呢？ 这时，现有的Placement机制，会汇总一个Resource Provider下面所有的某种资源的总和，即会查到的是主机上总内存，发现还有32GB，调度时，就认为这个主机可以作为备选主机。然而，实际我们从上图已经可以看出，实际每个NUMA CELL可供调度的内存均只有16GB，其实是不满足要求的。 3. 基础的数据模型在现有Resource Provider的基础上，实现这种嵌套关系，基础的数据模型非常重要。在关系数据库中，对分层数据进行管理，主要有2个模型： 邻接表（Adjacency list）。记录parent，即父节点。 嵌套集合（Nested sets）。记录left和right，注意这里不是指兄弟节点，而是类似一个编号，right=left+2n+1，根节点比较特殊，left为1，right为2n，其中，n为节点的总数。 有关邻接表和嵌套模型的内容可以参考Managing Hierarchical Data in MySQL和Join-fu: The Art of SQL。 Reousce Provider最终选择了邻接表作为基础数据结构，在etherpad中，轻描淡写的描述了下选择的原因： A simple way of modeling this kind of nesting in a relational data store is to use something called an adjacency list model. We add a NULLABLE parent_resource_provider_id column to the resource_providers table to indicate that the resource provider is either a “top-level” provider (such as a compute host) or a “nested” provider (such as a NUMA cell on the compute host). 个人认为，选择这个模型的原因除了实现比较简单外，还有就是Resource Provider嵌套层级的不是非常深，即使进行一些查询时需要left join几次，也不会有非常大的性能损耗。当然，正如Jaypipes的PPT所述那样，比起Nest Sets来说，Adjacency list是very common but doesn’t scale。 另外，Managing Hierarchical Data in MySQL的“LIMITATIONS OF THE ADJACENCY LIST MODEL”一节中，提到了2个这个数据结构的限制，回头看来，在Placement进行设计时，都有应对的措施： Working with the adjacency list model in pure SQL can be difficult at best. Before being able to see the full path of a category we have to know the level at which it resides. In addition, special care must be taken when deleting nodes because of the potential for orphaning an entire sub-tree in the process (delete the portable electronics category and all of its children are orphaned). 其一，是Full-path的遍历（例如，获取一个父树），我们必须要多次join，并且需要知道自己在第几层，从而决定join的次数。这个在Placement，加了一个root_id进行解决。其二，是删除父类节点时，有可能造成底下的树被孤立了，这个Placement则是通过限制用户行为来解决的，即不允许删除有子节点的父节点。 因此，最终在Resource Provider的模型中，我们新增了2个field： parent_provider_uuid：表示Resource Provider的直接父亲节点。对于非嵌套的节点，这个field为NULL，对于嵌套的节点来说，这个字段在大多数情况可能是计算节点的UUID，表示这个资源是host下的资源； Indicates the UUID of the immediate parent provider. This will be None for the vast majority of providers, and for nested resource providers, this will most likely be the compute host’s UUID. root_provider_uuid：表示这个Resource Provider是一个树形providers的根节点。这个节点可以允许我们实现一个高效的树形访问，从而避免递归地查询父子关系。 Indicates the UUID of the resource provider that is at the “root” of the tree of providers. This field allows us to implement efficient tree-access queries and avoid use of recursive queries to follow child-&gt;parent relations. 数据模型的Patch在这: patch/377138。 4. 核心流程解析4.1 创建/删除 Nest Resource Provider最简单的流程就是对Nest Resource Provider进行操作了，对于创建的流程来说，需要用户传递父亲节点，请求的格式类似：1234&#123; \"name\": \"Shared storage\", \"parent_provider_uuid\": \"542df8ed-9be2-49b9-b4db-6d3183ff8ec8\"&#125; 在创建的过程中，如果包含了父亲节点，那么，我们可以很方便的找到其对应的root节点，然后填到自己的root中（子节点和父节点有相同的root）；如果没有父节点的话，那么这个子节点的根节点就是自己了（参考代码：nova/objects/resource_provider.py#L812-L819）。 同样的，在删除节点的时候，也需要考虑下嵌套的关系，在Resource Provider删除时，加了一个简单的限制：如果一个Resource Provider有子节点(参考代码：nova/objects/resource_provider.py#L824-L830)，则不允许进行删除。 4.2 获取满足条件的Resource Provider在 #63 中，我们提到过Nested Resource Provider的获取流程，参考Patch/534968大致的过程有以下几步： 获取满足条件的所有树的Root id。在这一步中，对整棵树中的资源，进行了获取和判断。参考Patch/534866。 获取root id对应的树的usage信息。参考Patch/534967例如，对于上述的结构中，当用户进行请求时，则会将右边的树获取出来，然后最终拿到父亲节点。 由于目前Patch还在开发中，并且在估计最早要到Rocky版本才能完成，所以，等到全部完成后，再进行更详尽的介绍。 参考 nested-resource-providers的etherpad Managing Hierarchical Data in MySQL Join-fu: The Art of SQL","pubDate":"Tue, 26 Dec 2017 08:33:42 GMT","guid":"http://yikun.github.io/2017/12/26/Placement深度探索-Nested-Resource-Providers/","category":"Nova,OpenStack"},{"title":"[Placement深度探索] Get Allocation Candidates","link":"http://yikun.github.io/2017/12/25/Placement深度探索-Get-Allocation-Candidates/","description":"1 功能概述Placement的一个重要的接口，就是获取满足指定资源条件的allocation。举个例子，用户说，我需要1个VCPU，512MB内存，1GB磁盘的资源，Placement你帮我找找看看，有没有合适的资源。","pubDate":"Mon, 25 Dec 2017 09:25:17 GMT","guid":"http://yikun.github.io/2017/12/25/Placement深度探索-Get-Allocation-Candidates/","category":"Nova,OpenStack"},{"title":"一个死锁问题的深入探究","link":"http://yikun.github.io/2017/12/13/一个死锁问题的深入探究/","description":"本篇文章可以看做是一个问题定位、分析、学习过程的记录，介绍了OpenStack Nova一个死锁问题的分析和解决的过程，你将从本文了解到SQLAlchemy的session中的语序排序机制、OpenStack的死锁重试机制及改进点以及一些调试的手段。 0. 背景在Nova对虚拟机进行一些操作的时候，比如创建、停止虚拟机之类的操作的时候，会将这些事件记录在instance_actions表里面记录操作的时间、操作类型以及一些操作事件详情。 例如，我们可以通过instnace-action-list来查看虚拟机的操作，并可以通过对应的req id来查操作中的事件详情，如果是失败的话，还可以从事件详情中，看到对应的错误栈信息。12345678910111213141516171819202122232425$ nova instance-action-list e92885a9-06d6-4491-ac43-6fd04e32ee72+--------+------------------------------------------+---------+----------------------------+| Action | Request_ID | Message | Start_Time |+--------+------------------------------------------+---------+----------------------------+| create | req-416cb88e-5adb-4c0f-9c32-6370d3661940 | - | 2017-12-13T12:08:36.000000 || stop | req-52155da3-d2ca-463c-b380-6034c0b5fdf1 | - | 2017-12-13T12:09:17.000000 |+--------+------------------------------------------+---------+----------------------------+$ nova instance-action e92885a9-06d6-4491-ac43-6fd04e32ee72 req-52155da3-d2ca-463c-b380-6034c0b5fdf1+---------------+--------------------------------------------------+| Property | Value |+---------------+--------------------------------------------------+| action | stop || events | [&#123;u'event': u'compute_stop_instance', || | u'finish_time': u'2017-12-13T12:09:23.000000', || | u'result': u'Success', || | u'start_time': u'2017-12-13T12:09:18.000000', || | u'traceback': None&#125;] || instance_uuid | e92885a9-06d6-4491-ac43-6fd04e32ee72 || message | - || project_id | 0232cef222f7479fae3fd8fa24d8c382 || request_id | req-52155da3-d2ca-463c-b380-6034c0b5fdf1 || start_time | 2017-12-13T12:09:17.000000 || user_id | 5b0b6a4c068f4c1ba78b50d8a4db5057 |+---------------+--------------------------------------------------+ 在instance_action的表里面，记录着action的更新时间，比如event结束了，我们也期望能够action里面能记录update的时间，但是目前并没有进行刷新。 这个patch想做的事儿也比较简单，如上图所示，就是在event进行记录（比如开始和结束）的时候，也对action的更新时间也做刷新。也就是说，我们在写instance_event_action表后，也需要写instance_action表去记录下刷新时间。大致代码的关键逻辑如下所示（省略了一些无关的代码细节）：1234567891011121314151617@pick_context_manager_writerdef action_event_finish(context, values): \"\"\"Finish an event on an instance action.\"\"\" # 原有获取action action = _action_get_by_request_id(context, values['instance_uuid'], values['request_id']) # 原有获取event event_ref = model_query(context, models.InstanceActionEvent).\\ filter_by(action_id=action['id']).\\ filter_by(event=values['event']).\\ first() # 原有的event刷新流程 event_ref.update(values) # **新增的刷新action时间逻辑** action.update(&#123;'updated_at': values['finish_time']&#125;) action.save(context.session) return event_ref 1. 起因在修复Nova的这个事件时间刷新问题(bug/507473)的时候，CI会概率性地挂一些用例，先开始以为是CI不稳定，workflow+1之后，最终的门禁检查一直过不了。Matt recheck了几次都是失败的，然后问： I’m not sure if the test failures this patch is hitting are related to this change or not - they definitely don’t seem to be (I’m not sure why we’d get an UnexpectedTaskStateError during resize due to this change). 这才引起了我的注意，我找了下归档的日志发现： Exception during message handling: DBDeadlock: (pymysql.err.InternalError) (1213, u’Deadlock found when trying to get lock; try restarting transaction’) [SQL: u’UPDATE instance_actions SET updated_at=%(updated_at)s WHERE instance_actions.id = %(instance_actions_id)s‘] [parameters: {‘instance_actions_id’: 23, ‘updated_at’: datetime.datetime(2017, 12, 4, 2, 48, 36, 91068)}] 第一反应是，我去！死锁了？简单的一个update怎么会死锁？确认了下where条件比较单一，并不是因为条件排序不稳定引起的死锁；也确认了下action数据库的索引，也比较简单，也不会有死锁问题。然后，就看业务代码，代码逻辑也很简单，一个事务里面包含了4件事，2个查询，2个刷新。不科学啊！ 2. 发现遇到这种活久见的问题，最好的办法就是把每一句SQL都dump出来，因为不是裸写SQL，鬼知道SQLAlchemy中间的ORM那层为我们做了什么。 OpenStack的oslo.db为我们提供了一个配置项：123[database]# (Integer) Verbosity of SQL debugging information: 0=None, 100=Everything.connection_debug = 100 把他设置成100就可以dump出执行的每一句SQL了，这个方法在我们进行调试的时候很方便。然后，进行复现，结果让我震惊了（问号脸？？？代码是一样的，生成SQL的顺序却是不一致的）：12345678910111213141516171819-- 从API dump的结果BEGIN (implicit)SELECT ... FROM instance_actions WHERE ...SELECT ... FROM instance_actions_events WHERE ...-- 先刷新actionUPDATE instance_actions SET updated_at=%(updated_at)s WHERE instance_actions.id= %(instance_actions_id)s-- 再刷新action_eventUPDATE instance_actions_events SET updated_at=%(updated_at)s, finish_time=%(finish_time)s, result=%(result)s WHERE instance_actions_events.id = %(instance_actions_events_id)sCOMMIT-- 从Conductor dump的结果BEGIN (implicit)SELECT ... FROM instance_actions WHERE ...SELECT ... FROM instance_actions_events WHERE ...-- 先刷新action_eventUPDATE instance_actions_events SET updated_at=%(updated_at)s, finish_time=%(finish_time)s, result=%(result)s WHERE instance_actions_events.id = %(instance_actions_events_id)s-- 再刷新actionUPDATE instance_actions SET updated_at=%(updated_at)s WHERE instance_actions.id= %(instance_actions_id)sCOMMIT 完整的SQL dump我贴在了paste/628609，可以分析出来，就是产生死锁的根本原因：在一个事务中，更新2个表的相反行。并发执行2个这样的事务，一个事务拿着action表的行锁，一个事务拿着action_event表的行锁，它们都互相等着对方释放，最终产生了死锁，如下图所示。 从MySQL官方DOC里，给的建议How to Minimize and Handle Deadlocks中，我们也看到了类似的建议： When modifying multiple tables within a transaction, or different sets of rows in the same table, do those operations in a consistent order each time. Then transactions form well-defined queues and do not deadlock. For example, organize database operations into functions within your application, or call stored routines, rather than coding multiple similar sequences of INSERT, UPDATE, and DELETE statements in different places. 核心意思就是说，我们在一个transaction中更新多个表的时候，或者说在一个表中更新不同行的时候，一定要保证每一次调用的顺序是一致的。最终，临时解决这个问题的方式也比较简单，就是在这个函数上加一个死锁重试装饰器，即在发生死锁的时候进行重试，CI终于全绿了。 3. 进一步分析问题解决就结束了吗？不，2个疑问一直在心中徘徊： 死锁重试的装饰器是怎么实现的，真的有效吗？ SQLalchemy做了什么导致最终生成SQL的顺序是不稳定的，为什么要这么做？ 3.1 oslo.db的死锁重试机制这个问题的场景和上研的时候，在通信中搞的“退避算法”很类似（HINOC中信道接纳的时候，多个节点并行接纳时，如果发生冲突，需要退避重试），都是冲突避免，通信中是避免信道冲突，而这里则是避免数据库的死锁。 我们从oslo_db/api.py可以看到他的实现，原理比较简单，就是隔几秒（2的retry数次方秒），如果调用成功，就终止重试。伪代码大概如下：12345678910t = 1for i in range(try): sleep(t) # 指数递增 t = t * 2 # 超过上限取上限 t = min(max_t, t) func() if not raise deadlock: break 虽然看着隔了一些时间，但是，这种指数递增的机制对于死锁这种问题没有什么卵用，大家一起等，然后再一起调，还是会再次产生死锁。对于这个问题，我提交了一个Patch对其进行优化，具体内容可以参考 #71 《oslo.db中的死锁重试机制优化》的详细分析。 3.2 SQLAlchemy Session中的排序机制上文已经提到，造成死锁的根本原因实际上是在一个事务中，更新2个表的时候的顺序不一致。在并发调用的时候，产生了死锁。Python的代码是按顺序更新的（先更新event内容，再更新action），但是为什么SQLAlchemy产生的SQL是乱序的呢？ 通过阅读SQLAlchemy的源码，最终找到了答案。先说结论：Session中的操作顺序，由UnitOfWork机制决定最终的调用顺序，如果没有依赖关系，最终执行顺序是不稳定的。 3.2.1. SQLAlchemy的缓存刷新机制SQLAlchemy在进行数据刷新的时候，会有一个flush的过程(实现见lib/sqlalchemy/orm/session.py#def flush，这个过程会将所有的object的变化，刷新到数据库中。例如，会将插入、修改、删除，转换为INSERT、UPDATE、DELETE等操作。而刷新执行的顺序，是通过Session的”UNIT of Worker”依赖机制保证的。 我们可以从有SQLalchemy作者写的一篇关于其架构的文章《SQLAlchemy》中看到一些关于Session相关的数据结构： Session维护着如上图所示的结构，在每次刷新的时候，会将object的变动刷新到数据库中。如作者所说说，flush这个函数可能是 SQLAlchemy最复杂的函数。 3.2.2. SQLAlchemy的UNIT of WORK机制我们先看看来自作者的介绍： The job of the unit of work is to move all of the pending state present in a particular Session out to the database, emptying out the new, dirty, and deleted collections maintained by the Session. Once completed, the in-memory state of the Session and what’s present in the current transaction match. The primary challenge is to determine the correct series of persistence steps, and then to perform them in the correct order. UOW的工作主要是将session维护的new、dirty、deleted的集合清掉并落入数据库中。主要挑战就是决定正确的持久化步骤和顺序。我们看到了关键的地方，排序！ 从这篇文章中，我们了解到，其实对于UOW来说，共有两级排序：1） 第一级排序，是针对于多个表（class）之前的排序，依赖信息从表之间的关系获取，例如文章中所举的User和Address的例子，需要在user插入后，有了主键，然后再去更新。2）第二季排序，是针对于一个表（class）之中操作的排序，例如文章中所举的，前一个插入的user依赖后一个user。 然而，无论是哪个排序，如果表和表之间在SQLAlchemy定义模型的时候，并没有指定其顺序，那么便没有依赖关系，也便意味着，顺序是不稳定的。 在我们出现的问题中，action和action_event在model定义的代码中，并未指定action和event之前的关系，因此，SQLAlchemy分析依赖的时候，只是将这两个表当做独立的2个表。 3.2.3. 实战一把为了证明我们的分析，我们在SQLAlchemy打印一些日志来记录依赖关系和最终执行的结果，代码见lib/sqlalchemy/ormunitofwork.py，取消掉这些注释即可。 dependencies: set([(SaveUpdateAll(Mapper|InstanceActionEvent|instance_actions_events), DeleteAll(Mapper|InstanceActionEvent|instance_actions_events)), (SaveUpdateAll(Mapper|InstanceAction|instance_actions), DeleteAll(Mapper|InstanceAction|instance_actions))]) cycles: set([]) sort: [SaveUpdateAll(Mapper|InstanceAction|instance_actions), SaveUpdateAll(Mapper|InstanceActionEvent|instance_actions_events), DeleteAll(Mapper|InstanceActionEvent|instance_actions_events), DeleteAll(Mapper|InstanceAction|instance_actions)] COUNT OF POSTSORT ACTIONS 4 上面共4行信息，我们需要的是dependencies信息和sort信息，从依赖信息我们可以看到，我们进行的这个事务仅有2组依赖，分别是action和event_action的缓存入库先于缓存清空，而action和event_action之间是没有依赖关系的。所以，最终生成的sort列表，其实是无法保证稳定性的。 所以，才会出现我们本文所出的问题，一会先刷新action，一会先刷新action_event。然而，对于这种问题并不是无解，我们只需要在这两个表里加入relationship，使他们有依赖就可以了。如果确实没有什么关联，那我们就需要思考把更新拆分到更小的事务中了，就像MySQL官网说的那样：Keep transactions small and short in duration to make them less prone to collision。 4. 总结。TL;DR。写完这篇文章发现，有点太长了，不想细看的看看总结吧，哈哈。 遇到OpenStack数据库相关问题，可以通过设置[database]/connection_debug=100进行SQL打印。 SQLAlchemy对于一个session中的更新顺序，如果表之间没有依赖，是无法保证顺序的。 在一个事务中，更新多张表，需要考虑顺序，若ORM无法保证的更新顺序，尽量不要放在同一个事务中，尽量确保事务做的事简单。 oslo.db目前的死锁重试机制，是大家一起等X秒，很有可能再次死锁。 参考 Instance action’s updated_at issue How to Minimize and Handle Deadlocks Exponential Backoff And Jitter SQLAlchemy library(tutorials, arch doc, talks, posts Some discussion on backoff algorithm Is SQLAlchemy saves order in adding objects to session? SQLAlchemy at Architecture of Open Source Applications","pubDate":"Wed, 13 Dec 2017 11:47:10 GMT","guid":"http://yikun.github.io/2017/12/13/一个死锁问题的深入探究/","category":"Nova,OpenStack,Python"},{"title":"Nova调度相关特性理解与梳理","link":"http://yikun.github.io/2017/12/06/Nova调度相关特性理解与梳理/","description":"准备拿这篇文章梳理下OpenStack Nova调度相关的特性，由于目前Placement的引入，说起调度，和这个组件是分不开的，所以本文也可以看做是Placement的一个历史特性的梳理。第一阶段会按照版本，先把调度相关的BP过一遍，然后再通过理解和使用加强理解。好吧，我承认又开了一个系列的坑，话不多说，开始！","pubDate":"Wed, 06 Dec 2017 01:41:11 GMT","guid":"http://yikun.github.io/2017/12/06/Nova调度相关特性理解与梳理/","category":"Nova,OpenStack"}]}